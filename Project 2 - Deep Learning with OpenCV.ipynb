{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f91c802a-ccce-41c2-b137-8a1d6d2e37ad",
   "metadata": {},
   "source": [
    "# Mini-project 1:\n",
    "Write a program that recognizes several different body positions. (Assume a person is in the image)\n",
    "\n",
    "To simplify the problem, you can focus on two positions: standing and sitting. If you are interested, you can add positions.\n",
    "\n",
    "Depending on your taste, you can use a classifier, but it is not required.\n",
    "\n",
    "Please work on the program on the video and write your prediction on each frame.\n",
    "\n",
    "Test the output on a small video and send the code along with the input video.\n",
    "\n",
    "Similar work to this article:\n",
    "\n",
    "https://ieeexplore.ieee.org/abstract/document/9116911/\n",
    "\n",
    "# Mini-project 2:\n",
    "Write a program that compares all faces on a webcam or video with a reference face and marks the faces that are different from the reference face with a red rectangle and the person who is the same as the reference image with a green rectangle.\n",
    "\n",
    "# Mini-project 3:\n",
    "a) Write a program that first finds a desired object with an object detection model and then tracks it with an object tracking method that we learned in the previous module.\n",
    "\n",
    "b) Search the Internet to find a method that covers the shortcomings of the method you wrote in the first section. What is the difference?\n",
    "\n",
    "(Hint: What happens if the object goes off the page in your method? What is the solution?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6eb30db3-d4b6-47ab-a524-f4b5c9a5c87f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\matin\\anaconda3\\lib\\site-packages (4.11.0.86)\n",
      "Requirement already satisfied: mediapipe in c:\\users\\matin\\anaconda3\\lib\\site-packages (0.10.21)\n",
      "Requirement already satisfied: numpy in c:\\users\\matin\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: absl-py in c:\\users\\matin\\anaconda3\\lib\\site-packages (from mediapipe) (2.1.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\matin\\anaconda3\\lib\\site-packages (from mediapipe) (24.3.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\matin\\anaconda3\\lib\\site-packages (from mediapipe) (24.12.23)\n",
      "Requirement already satisfied: jax in c:\\users\\matin\\anaconda3\\lib\\site-packages (from mediapipe) (0.6.0)\n",
      "Requirement already satisfied: jaxlib in c:\\users\\matin\\anaconda3\\lib\\site-packages (from mediapipe) (0.6.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\matin\\anaconda3\\lib\\site-packages (from mediapipe) (3.10.0)\n",
      "Requirement already satisfied: opencv-contrib-python in c:\\users\\matin\\appdata\\roaming\\python\\python312\\site-packages (from mediapipe) (4.11.0.86)\n",
      "Requirement already satisfied: protobuf<5,>=4.25.3 in c:\\users\\matin\\anaconda3\\lib\\site-packages (from mediapipe) (4.25.3)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in c:\\users\\matin\\anaconda3\\lib\\site-packages (from mediapipe) (0.5.1)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\matin\\anaconda3\\lib\\site-packages (from mediapipe) (0.2.0)\n",
      "Requirement already satisfied: CFFI>=1.0 in c:\\users\\matin\\anaconda3\\lib\\site-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
      "Requirement already satisfied: ml_dtypes>=0.5.0 in c:\\users\\matin\\anaconda3\\lib\\site-packages (from jax->mediapipe) (0.5.1)\n",
      "Requirement already satisfied: opt_einsum in c:\\users\\matin\\anaconda3\\lib\\site-packages (from jax->mediapipe) (3.4.0)\n",
      "Requirement already satisfied: scipy>=1.11.1 in c:\\users\\matin\\anaconda3\\lib\\site-packages (from jax->mediapipe) (1.13.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\matin\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\matin\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\matin\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\matin\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\matin\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\matin\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\matin\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\matin\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (2.9.0.post0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\matin\\anaconda3\\lib\\site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.21)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\matin\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install opencv-python mediapipe numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd39dec-9b2a-453a-aa98-dc1536be1ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "# Initialize MediaPipe\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "class BodyPositionRecognizer:\n",
    "    def __init__(self):\n",
    "        self.pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.7)\n",
    "        self.hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.5)\n",
    "        self.next_person_id = 0\n",
    "        self.people_data = {}\n",
    "        \n",
    "        # Constants\n",
    "        self.HANDSHAKE_DIST_THRESH = 0.15\n",
    "        self.GREETING_DIST_THRESH = 0.25\n",
    "        self.OBJECT_HOLD_DIST_THRESH = 0.1\n",
    "        self.HISTORY_LENGTH = 10\n",
    "    \n",
    "    def calculate_body_metrics(self, landmarks):\n",
    "        \"\"\"Calculate key body metrics for position detection\"\"\"\n",
    "        left_shoulder = landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value]\n",
    "        right_shoulder = landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value]\n",
    "        left_hip = landmarks[mp_pose.PoseLandmark.LEFT_HIP.value]\n",
    "        right_hip = landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value]\n",
    "        left_knee = landmarks[mp_pose.PoseLandmark.LEFT_KNEE.value]\n",
    "        right_knee = landmarks[mp_pose.PoseLandmark.RIGHT_KNEE.value]\n",
    "        left_ankle = landmarks[mp_pose.PoseLandmark.LEFT_ANKLE.value]\n",
    "        left_elbow = landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value]\n",
    "        left_wrist = landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value]\n",
    "        right_wrist = landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value]\n",
    "        nose = landmarks[mp_pose.PoseLandmark.NOSE.value]\n",
    "        \n",
    "        # Calculate key metrics\n",
    "        shoulder_hip_ratio = ((left_shoulder.y + right_shoulder.y) / \n",
    "                            (left_hip.y + right_hip.y))\n",
    "        hip_knee_ratio = ((left_hip.y + right_hip.y) / \n",
    "                         (left_knee.y + right_knee.y))\n",
    "        torso_angle = np.degrees(np.arctan2(left_hip.y - left_shoulder.y, \n",
    "                                          left_hip.x - left_shoulder.x))\n",
    "        arm_angle = np.degrees(np.arctan2(left_elbow.y - left_shoulder.y,\n",
    "                                        left_elbow.x - left_shoulder.x))\n",
    "        \n",
    "        # Hand to face distance for greeting detection\n",
    "        hand_to_face_dist = min(\n",
    "            np.sqrt((left_wrist.x - nose.x)**2 + (left_wrist.y - nose.y)**2),\n",
    "            np.sqrt((right_wrist.x - nose.x)**2 + (right_wrist.y - nose.y)**2)\n",
    "        )\n",
    "        \n",
    "        # Wrist to elbow distance for object holding\n",
    "        left_wrist_elbow_dist = np.sqrt((left_wrist.x - left_elbow.x)**2 + \n",
    "                                      (left_wrist.y - left_elbow.y)**2)\n",
    "        right_wrist_elbow_dist = np.sqrt((right_wrist.x - left_elbow.x)**2 + \n",
    "                                       (right_wrist.y - left_elbow.y)**2)\n",
    "        \n",
    "        return {\n",
    "            'shoulder_hip_ratio': shoulder_hip_ratio,\n",
    "            'hip_knee_ratio': hip_knee_ratio,\n",
    "            'torso_angle': torso_angle,\n",
    "            'arm_angle': arm_angle,\n",
    "            'ankle_y': left_ankle.y,\n",
    "            'hand_to_face_dist': hand_to_face_dist,\n",
    "            'wrist_elbow_dist': (left_wrist_elbow_dist + right_wrist_elbow_dist)/2,\n",
    "            'wrist_y': min(left_wrist.y, right_wrist.y)\n",
    "        }\n",
    "    \n",
    "    def detect_interactions(self, person1, person2):\n",
    "        \"\"\"Detect interactions between two people\"\"\"\n",
    "        if not person1['hand_landmarks'] or not person2['hand_landmarks']:\n",
    "            return None\n",
    "        \n",
    "        p1_wrist = person1['hand_landmarks'][0].landmark[mp_hands.HandLandmark.WRIST]\n",
    "        p2_wrist = person2['hand_landmarks'][0].landmark[mp_hands.HandLandmark.WRIST]\n",
    "        \n",
    "        distance = np.sqrt((p1_wrist.x - p2_wrist.x)**2 + \n",
    "                          (p1_wrist.y - p2_wrist.y)**2)\n",
    "        \n",
    "        if distance < self.HANDSHAKE_DIST_THRESH:\n",
    "            return \"Handshake\"\n",
    "        elif distance < self.GREETING_DIST_THRESH:\n",
    "            return \"Greeting\"\n",
    "        return None\n",
    "    \n",
    "    def detect_hand_gestures(self, metrics, hand_landmarks):\n",
    "        \"\"\"Detect hand gestures and positions\"\"\"\n",
    "        gestures = []\n",
    "        \n",
    "        # Check for hand raising\n",
    "        if metrics['wrist_y'] < 0.3:  # Normalized y-coordinate threshold\n",
    "            gestures.append(\"Hands Raised\")\n",
    "            \n",
    "        # Check for greeting (hand near face)\n",
    "        if metrics['hand_to_face_dist'] < 0.2:\n",
    "            gestures.append(\"Greeting\")\n",
    "            \n",
    "        # Check for object holding (closed fist)\n",
    "        if hand_landmarks:\n",
    "            for hand in hand_landmarks:\n",
    "                # Calculate distance between wrist and middle finger tip\n",
    "                wrist = hand.landmark[mp_hands.HandLandmark.WRIST]\n",
    "                middle_tip = hand.landmark[mp_hands.HandLandmark.MIDDLE_FINGER_TIP]\n",
    "                dist = np.sqrt((wrist.x - middle_tip.x)**2 + (wrist.y - middle_tip.y)**2)\n",
    "                \n",
    "                if dist < self.OBJECT_HOLD_DIST_THRESH:\n",
    "                    gestures.append(\"Holding Object\")\n",
    "                    break\n",
    "                    \n",
    "        return gestures\n",
    "    \n",
    "    def detect_body_position(self, landmarks, person_id, hand_landmarks=None):\n",
    "        \"\"\"Detect body position with multiple states\"\"\"\n",
    "        metrics = self.calculate_body_metrics(landmarks)\n",
    "        gestures = self.detect_hand_gestures(metrics, hand_landmarks)\n",
    "        \n",
    "        if person_id not in self.people_data:\n",
    "            self.people_data[person_id] = {\n",
    "                'history': deque(maxlen=self.HISTORY_LENGTH),\n",
    "                'hand_landmarks': None,\n",
    "                'position_history': deque(maxlen=5)\n",
    "            }\n",
    "        \n",
    "        current_pos = (landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].x,\n",
    "                      landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].y)\n",
    "        self.people_data[person_id]['history'].append(current_pos)\n",
    "        self.people_data[person_id]['hand_landmarks'] = hand_landmarks\n",
    "        \n",
    "        # Improved sitting detection using multiple metrics\n",
    "        sitting_condition = (\n",
    "            metrics['hip_knee_ratio'] < 1.2 and\n",
    "            metrics['ankle_y'] > 0.85 and\n",
    "            abs(metrics['torso_angle']) < 30\n",
    "        )\n",
    "        \n",
    "        if sitting_condition:\n",
    "            position = \"Sitting\"\n",
    "        elif abs(metrics['torso_angle']) > 45:\n",
    "            position = \"Bending\"\n",
    "        elif metrics['shoulder_hip_ratio'] > 1.25:\n",
    "            if len(self.people_data[person_id]['history']) == self.HISTORY_LENGTH:\n",
    "                movement = sum(\n",
    "                    np.sqrt((self.people_data[person_id]['history'][i][0] - \n",
    "                            self.people_data[person_id]['history'][i-1][0])**2 + \n",
    "                           (self.people_data[person_id]['history'][i][1] - \n",
    "                            self.people_data[person_id]['history'][i-1][1])**2)\n",
    "                    for i in range(1, self.HISTORY_LENGTH))\n",
    "                if movement > 0.08:\n",
    "                    position = \"Walking\"\n",
    "                else:\n",
    "                    position = \"Standing\"\n",
    "            else:\n",
    "                position = \"Standing\"\n",
    "        elif metrics['shoulder_hip_ratio'] < 0.9:\n",
    "            position = \"Lying Down\"\n",
    "        else:\n",
    "            position = \"Unknown\"\n",
    "        \n",
    "        # Combine position with gestures\n",
    "        if gestures:\n",
    "            position += \" + \" + \" + \".join(gestures)\n",
    "        \n",
    "        # Store position history for smoothing\n",
    "        self.people_data[person_id]['position_history'].append(position)\n",
    "        \n",
    "        # Get most frequent position in history\n",
    "        if len(self.people_data[person_id]['position_history']) > 0:\n",
    "            position = max(set(self.people_data[person_id]['position_history']), \n",
    "                          key=self.people_data[person_id]['position_history'].count)\n",
    "        \n",
    "        return position\n",
    "\n",
    "    def process_frame(self, frame):\n",
    "        \"\"\"Process a single frame and return annotated frame\"\"\"\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        pose_results = self.pose.process(frame_rgb)\n",
    "        hand_results = self.hands.process(frame_rgb)\n",
    "        \n",
    "        current_people = {}\n",
    "        \n",
    "        if pose_results.pose_landmarks:\n",
    "            person_id = self.next_person_id\n",
    "            self.next_person_id += 1\n",
    "            \n",
    "            # Associate hands with person (simplified approach)\n",
    "            person_hand_landmarks = []\n",
    "            if hand_results.multi_hand_landmarks:\n",
    "                for hand in hand_results.multi_hand_landmarks:\n",
    "                    hand_x = hand.landmark[mp_hands.HandLandmark.WRIST].x\n",
    "                    hand_y = hand.landmark[mp_hands.HandLandmark.WRIST].y\n",
    "                    body_x = pose_results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_SHOULDER].x\n",
    "                    body_y = pose_results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_SHOULDER].y\n",
    "                    \n",
    "                    if abs(hand_x - body_x) < 0.3 and abs(hand_y - body_y) < 0.3:\n",
    "                        person_hand_landmarks.append(hand)\n",
    "            \n",
    "            position = self.detect_body_position(\n",
    "                pose_results.pose_landmarks.landmark, \n",
    "                person_id,\n",
    "                person_hand_landmarks)\n",
    "            \n",
    "            current_people[person_id] = {\n",
    "                'position': position,\n",
    "                'landmarks': pose_results.pose_landmarks,\n",
    "                'hand_landmarks': person_hand_landmarks\n",
    "            }\n",
    "            \n",
    "            # Draw pose landmarks\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame, pose_results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "            \n",
    "            # Draw hand landmarks\n",
    "            if person_hand_landmarks:\n",
    "                for hand in person_hand_landmarks:\n",
    "                    mp_drawing.draw_landmarks(\n",
    "                        frame, hand, mp_hands.HAND_CONNECTIONS)\n",
    "            \n",
    "            # Choose color based on position\n",
    "            color = (0, 255, 0)  # Default green\n",
    "            if \"Sitting\" in position:\n",
    "                color = (0, 0, 255)  # Red\n",
    "            elif \"Walking\" in position:\n",
    "                color = (255, 255, 0)  # Cyan\n",
    "            elif \"Bending\" in position:\n",
    "                color = (0, 255, 255)  # Yellow\n",
    "            elif \"Lying Down\" in position:\n",
    "                color = (255, 0, 255)  # Purple\n",
    "            elif \"Hands Raised\" in position:\n",
    "                color = (255, 165, 0)  # Orange\n",
    "            elif \"Holding Object\" in position:\n",
    "                color = (0, 165, 255)  # Blue\n",
    "            elif \"Greeting\" in position:\n",
    "                color = (255, 192, 203)  # Pink\n",
    "            \n",
    "            # Display position\n",
    "            cv2.putText(frame, f\"Person {person_id}: {position}\", \n",
    "                       (10, 30 + person_id * 30),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "        \n",
    "        # Detect interactions between people\n",
    "        if len(current_people) >= 2:\n",
    "            person_ids = list(current_people.keys())\n",
    "            interaction = self.detect_interactions(\n",
    "                current_people[person_ids[0]],\n",
    "                current_people[person_ids[1]])\n",
    "            \n",
    "            if interaction:\n",
    "                cv2.putText(frame, f\"Interaction: {interaction}\", \n",
    "                           (frame.shape[1]//2 - 100, 30),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "        \n",
    "        return frame\n",
    "\n",
    "def process_video(input_video, output_video):\n",
    "    recognizer = BodyPositionRecognizer()\n",
    "    \n",
    "    cap = cv2.VideoCapture(input_video)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open video {input_video}\")\n",
    "        return\n",
    "    \n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_video, fourcc, fps, (width, height))\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        success, frame = cap.read()\n",
    "        if not success:\n",
    "            break\n",
    "            \n",
    "        processed_frame = recognizer.process_frame(frame)\n",
    "        out.write(processed_frame)\n",
    "        \n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Example usage\n",
    "input_vid = r'D:\\exam\\vtest.avi'\n",
    "output_vid = \"output_video_enhanced.mp4\"\n",
    "process_video(input_vid, output_vid)\n",
    "print(f\"Processing complete. Output saved to {output_vid}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11126da7-3a83-43e0-8c3b-0fc1e96bdbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vid = r'D:\\exam\\vtest.avi'\n",
    "output_vid = \"output_video_enhanced.mp4\"\n",
    "process_video(input_vid, output_vid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d8991a-adac-4424-83c0-9942e460244c",
   "metadata": {},
   "source": [
    "# Mini-project 2:\n",
    "Write a program that compares all faces on a webcam or video with a reference face and marks the faces that are different from the reference face with a red rectangle and the person who is the same as the reference image with a green rectangle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "741d2fec-434f-490f-9d15-dad5275ccad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# تنظیمات اولیه\n",
    "REFERENCE_IMAGE_PATH = \"reference.jpg\"  # مسیر تصویر مرجع\n",
    "MODEL_PATH = \"C:/Users/Matin/face_detection_yunet_2023mar.onnx\"  # مسیر مدل تشخیص چهره\n",
    "RECOGNIZER_PATH = \"C:/Users/Matin/face_recognition_sface_2021dec.onnx\"  # مسیر مدل تشخیص هویت\n",
    "\n",
    "# آستانه‌های شباهت\n",
    "L2_THRESHOLD = 1.128\n",
    "COSINE_THRESHOLD = 0.363\n",
    "\n",
    "# بارگذاری مدل‌ها\n",
    "detector = cv2.FaceDetectorYN.create(MODEL_PATH, \"C:/Users/Matin/face_detection_yunet_2023mar.onnx\", (320, 320), 0.9, 0.3, 5000)\n",
    "recognizer = cv2.FaceRecognizerSF.create(RECOGNIZER_PATH, \"C:/Users/Matin/face_recognition_sface_2021dec.onnx\")\n",
    "\n",
    "# بارگذاری تصویر مرجع و استخراج ویژگی‌ها\n",
    "ref_image = cv2.imread(r'D:\\exam\\refrence_image.jpg')\n",
    "if ref_image is None:\n",
    "    raise ValueError(\"تصویر مرجع یافت نشد\")\n",
    "\n",
    "detector.setInputSize((ref_image.shape[1], ref_image.shape[0]))\n",
    "ref_faces = detector.detect(ref_image)\n",
    "\n",
    "if ref_faces[1] is None:\n",
    "    raise ValueError(\"هیچ چهره‌ای در تصویر مرجع یافت نشد\")\n",
    "\n",
    "ref_face_align = recognizer.alignCrop(ref_image, ref_faces[1][0])\n",
    "ref_feature = recognizer.feature(ref_face_align)\n",
    "\n",
    "# راه‌اندازی وبکم\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    raise RuntimeError(\"وبکم راه‌اندازی نشد\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # تشخیص چهره‌ها در فریم\n",
    "    height, width = frame.shape[:2]\n",
    "    detector.setInputSize((width, height))\n",
    "    faces = detector.detect(frame)\n",
    "    \n",
    "    if faces[1] is not None:\n",
    "        for face in faces[1]:\n",
    "            # ترازبندی و استخراج ویژگی چهره\n",
    "            face_align = recognizer.alignCrop(frame, face)\n",
    "            face_feature = recognizer.feature(face_align)\n",
    "            \n",
    "            # محاسبه شباهت با تصویر مرجع\n",
    "            l2_score = recognizer.match(ref_feature, face_feature, cv2.FaceRecognizerSF_FR_NORM_L2)\n",
    "            cosine_score = recognizer.match(ref_feature, face_feature, cv2.FaceRecognizerSF_FR_COSINE)\n",
    "            \n",
    "            # تعیین رنگ مستطیل بر اساس شباهت\n",
    "            if l2_score <= L2_THRESHOLD and cosine_score >= COSINE_THRESHOLD:\n",
    "                color = (0, 255, 0)  # سبز برای چهره مشابه\n",
    "                label = \"Match\"\n",
    "            else:\n",
    "                color = (0, 0, 255)  # قرمز برای چهره متفاوت\n",
    "                label = \"Unknown\"\n",
    "            \n",
    "            # رسم مستطیل و برچسب\n",
    "            coords = face[:-1].astype(np.int32)\n",
    "            cv2.rectangle(frame, (coords[0], coords[1]), \n",
    "                         (coords[0]+coords[2], coords[1]+coords[3]), color, 2)\n",
    "            cv2.putText(frame, label, (coords[0], coords[1]-10), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)\n",
    "    \n",
    "    # نمایش فریم\n",
    "    cv2.imshow('Face Recognition', frame)\n",
    "    \n",
    "    # خروج با کلید ESC\n",
    "    if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "# آزادسازی منابع\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826c9774-ad81-49b6-9cf8-75e2b6659257",
   "metadata": {},
   "source": [
    "# Mini-project 3:\n",
    "a) Write a program that first finds a desired object with an object detection model and then tracks it with an object tracking method that we learned in the previous module.\n",
    "\n",
    "b) Search the Internet to find a method that covers the shortcomings of the method you wrote in the first section. What is the difference?\n",
    "\n",
    "(Hint: What happens if the object goes off the page in your method? What is the solution?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c209f92b-47f6-49b8-b652-ecbcc64aa1f2",
   "metadata": {},
   "source": [
    "# Part a: Object Detection and Tracking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8012ade2-8329-4abe-9b9c-1ed7c305e150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No objects detected!\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Initialize object detection model (YOLOv3)\n",
    "net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
    "classes = []\n",
    "with open(\"coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i-1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "# Initialize tracker (using CSRT)\n",
    "tracker = cv2.legacy.TrackerCSRT.create()\n",
    "\n",
    "# Video capture\n",
    "cap = cv2.VideoCapture(\"D:/exam/race_car.mp4\")\n",
    "\n",
    "# Detection phase\n",
    "ret, frame = cap.read()\n",
    "height, width = frame.shape[:2]\n",
    "\n",
    "# Detect objects using YOLO\n",
    "blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "net.setInput(blob)\n",
    "outs = net.forward(output_layers)\n",
    "\n",
    "# Process detections\n",
    "class_ids = []\n",
    "confidences = []\n",
    "boxes = []\n",
    "for out in outs:\n",
    "    for detection in out:\n",
    "        scores = detection[5:]\n",
    "        class_id = np.argmax(scores)\n",
    "        confidence = scores[class_id]\n",
    "        if confidence > 0.5 and class_id == 0:  # class_id 0 is for person\n",
    "            center_x = int(detection[0] * width)\n",
    "            center_y = int(detection[1] * height)\n",
    "            w = int(detection[2] * width)\n",
    "            h = int(detection[3] * height)\n",
    "            x = int(center_x - w / 2)\n",
    "            y = int(center_y - h / 2)\n",
    "            boxes.append([x, y, w, h])\n",
    "            confidences.append(float(confidence))\n",
    "            class_ids.append(class_id)\n",
    "\n",
    "# Apply non-max suppression\n",
    "indices = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "\n",
    "# Select first detected object to track\n",
    "if len(indices) > 0:\n",
    "    bbox = boxes[indices[0]]\n",
    "    tracker.init(frame, tuple(bbox))\n",
    "else:\n",
    "    print(\"No objects detected!\")\n",
    "    exit()\n",
    "\n",
    "# Tracking phase\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Update tracker\n",
    "    success, bbox = tracker.update(frame)\n",
    "    \n",
    "    # Draw bounding box if tracking was successful\n",
    "    if success:\n",
    "        x, y, w, h = [int(v) for v in bbox]\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "    else:\n",
    "        cv2.putText(frame, \"Tracking failure\", (100, 80), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 0, 255), 2)\n",
    "    \n",
    "    cv2.imshow(\"Tracking\", frame)\n",
    "    \n",
    "    # Exit if ESC pressed\n",
    "    if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bd5039-4700-466d-b336-2dfb185b07d5",
   "metadata": {},
   "source": [
    "# Part b: Improved Method with Re-detection Capabilit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1978e11-09b9-4af5-b20f-a603efd2ec7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Initialize YOLO\n",
    "net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
    "classes = []\n",
    "with open(\"coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i-1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "# Initialize tracker (using CSRT)\n",
    "tracker = cv2.legacy.TrackerCSRT.create()\n",
    "tracking = False\n",
    "frames_since_last_detection = 0\n",
    "DETECTION_INTERVAL = 30  # Re-detect every 30 frames\n",
    "\n",
    "cap = cv2.VideoCapture(\"D:/exam/race_car.mp4\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    if not tracking or frames_since_last_detection >= DETECTION_INTERVAL:\n",
    "        # Detection phase\n",
    "        height, width = frame.shape[:2]\n",
    "        blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "        net.setInput(blob)\n",
    "        outs = net.forward(output_layers)\n",
    "        \n",
    "        # Process detections\n",
    "        boxes = []\n",
    "        confidences = []\n",
    "        for out in outs:\n",
    "            for detection in out:\n",
    "                scores = detection[5:]\n",
    "                class_id = np.argmax(scores)\n",
    "                confidence = scores[class_id]\n",
    "                if confidence > 0.5 and class_id == 0:  # Person class\n",
    "                    center_x = int(detection[0] * width)\n",
    "                    center_y = int(detection[1] * height)\n",
    "                    w = int(detection[2] * width)\n",
    "                    h = int(detection[3] * height)\n",
    "                    x = int(center_x - w / 2)\n",
    "                    y = int(center_y - h / 2)\n",
    "                    boxes.append([x, y, w, h])\n",
    "                    confidences.append(float(confidence))\n",
    "        \n",
    "        # Apply non-max suppression\n",
    "        indices = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "        \n",
    "        if len(indices) > 0:\n",
    "            bbox = boxes[indices[0]]\n",
    "            tracker = cv2.legacy.TrackerCSRT.create()  # Reinitialize tracker\n",
    "            tracker.init(frame, tuple(bbox))\n",
    "            tracking = True\n",
    "            frames_since_last_detection = 0\n",
    "        else:\n",
    "            tracking = False\n",
    "    else:\n",
    "        # Tracking phase\n",
    "        success, bbox = tracker.update(frame)\n",
    "        frames_since_last_detection += 1\n",
    "        \n",
    "        if success:\n",
    "            x, y, w, h = [int(v) for v in bbox]\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        else:\n",
    "            tracking = False\n",
    "    \n",
    "    cv2.imshow(\"Improved Tracking\", frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d3995c-196a-4920-8337-77051ad34e3d",
   "metadata": {},
   "source": [
    "# مقایسه روش‌های ردیابی شیء و بهبودها\n",
    "\n",
    "## روش اولیه (ردیابی ساده)\n",
    "```python\n",
    "# کد پیاده‌سازی\n",
    "tracker = cv2.legacy.TrackerCSRT.create()\n",
    "success, bbox = tracker.update(frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62e63ab-bfee-4322-bbc8-3636e49fb3db",
   "metadata": {},
   "source": [
    "# Differences and Improvements\n",
    "\n",
    "## Problem with Initial Method:\n",
    "- If the object leaves the frame, tracking fails permanently.\n",
    "- No recovery mechanism when tracking fails.\n",
    "- Drift can accumulate over time.\n",
    "\n",
    "## Improved Method Features:\n",
    "- **Re-detection capability**: Performs object detection periodically (every 30 frames).\n",
    "- **Tracking recovery**: Can re-acquire the object if it returns to the frame.\n",
    "- **Adaptive switching**: Automatically switches between detection and tracking modes.\n",
    "- **Tracker reinitialization**: Creates a fresh tracker after each detection.\n",
    "\n",
    "## Key Differences:\n",
    "\n",
    "| Feature               | Initial Method | Improved Method |\n",
    "|-----------------------|----------------|-----------------|\n",
    "| Handles occlusions    | ❌ No          | ✅ Yes          |\n",
    "| Recovers from loss    | ❌ No          | ✅ Yes          |\n",
    "| Periodic re-detection | ❌ No          | ✅ Yes          |\n",
    "| Computational cost    | Lower          | Higher          |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
